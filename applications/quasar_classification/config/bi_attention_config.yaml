generic:
  name: 'bi_directional_attention_LSTM'

train:
  model_dir: 'model/bi_attention_model'
  steps: 10000
  batch_size: 32
  lr: 0.00002
model:
  is_bidirectional: Y
  is_attended: Y
  hidden_units: [128, 128]
  in_dropout: 0.9
  out_dropout: 0.8